{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6wyRwoV7OimSqs6aCI4WF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabh1911007/ML-Study-Jam/blob/main/ML_Study_Jam_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmQp9qf7MK8i"
      },
      "outputs": [],
      "source": [
        "# REINFORCEMENT LEARNING\n",
        "Motivation and Background\n",
        "\n",
        "one of my favourite quote about machine learning is from an UNKNOWN PERSON- \" As long as you can convert it into numbers you can apply ML to it \"\n",
        "\n",
        "INTRODUCTION\n",
        "The Reinforcement Learning is- in which an agent is learning how to navigate some environment.\n",
        "\n",
        "Atari games from the 1970-80's. The agent does not know anything about the game and must learn how to play it from trial and error. The only information that is available to the agent is the screen output of the game, and whether the previous action resulted in a reward or penalty.\n",
        "\n",
        "This is a difficult problem in Machine Learning /AI , because the agent must both learn to distinguish features in the game-images, and then connect the occurence of certain features in the game-images with its own actions and a reward or penalty that may be deferred many steps into the future.\n",
        "\n",
        "The basic idea is to have the agent estimate Q-values whenever it sees an image from the game-environment.\n",
        "\n",
        "The Q-values tell the agent which action is most likely to lead to the highest cumulative reward in the future.\n",
        "\n",
        "The problem is then reduced to finding these Q-values and storing them for later retrieval using a function approximator.\n",
        "\n",
        "Disc131-WallSmash.png\n",
        "\n",
        "THE PROBLEM\n",
        "the player or agent is supposed to hit a ball with a paddle, thus avoiding death while scoring points when the ball smashes pieces of a wall.\n",
        "\n",
        "When a human learns to play a game like this, the first thing to figure out is what part of the game environment you are controlling-\n",
        "\n",
        "in this case the paddle at the bottom. If you move right on the joystick then the paddle moves right and vice versa.\n",
        "\n",
        "The next thing is to figure out what the goal of the game is - in this case to smash as many bricks in the wall as possible so as to maximize the score.\n",
        "\n",
        "Finally you need to learn what to avoid - in this case you must avoid dying by letting the ball pass beside the paddle.\n",
        "\n",
        "16_problem.png\n",
        "\n",
        "The problem is that there are 10 states between the ball going downwards and the paddle hitting the ball, and there are an additional 18 states before the reward is obtained when the ball hits the wall and smashes some bricks. How can we teach an agent to connect these three situations and generalize to similar situations? The answer is to use so-called Reinforcement Learning with a Neural Network.\n",
        "\n",
        "Q-Values\n",
        "The Q-values indicate which action is expected to result in the highest future reward, thus telling the agent which action to take.\n",
        "\n",
        "Unfortunately we do not know what the Q-values are supposed to be, so we have to estimate them somehow. The Q-values are all initialized to zero and then updated repeatedly as new information is collected from the agent playing the game. When the agent scores a point then the Q-value must be updated with the new information.\n",
        "\n",
        "Q-value for state and action = reward + discount * max Q-value for next state\n",
        "\n",
        "MOTION TRACE\n",
        "If we only use a single image from the game-environment then we cannot tell which direction the ball is moving. The typical solution is to use multiple consecutive images to represent the state of the game-environment.\n",
        "\n",
        "This implementation uses another approach by processing the images from the game-environment in a motion-tracer that outputs two images. The left image is from the game-environment and the right image is the processed image, which shows traces of recent movements in the environment. In this case we can see that the ball is going downwards and has bounced off the right wall, and that the paddle has moved from the left to the right side of the screen.\n",
        "\n",
        "16_motion-trace.png\n",
        "\n",
        "IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym              # ! exclamation mark is to take the input in colab file and gym is like, what keras is to machine learning, keras gives the power to import any type of model so this means that gym basically gives the power to import any type of game enviroment for the model like atari game enviroment\n",
        "!apt-get install python-opengl -y     # opengl is lib for to render the images together so that it can be shown as motion or video or game\n",
        "!apt install xvfb -y          #X Virtual Framebuffer same as opengl\n",
        "!pip install pyvirtualdisplay      # pyvirtualdisplay gives the power to virtual display on another window in colab\n",
        "!pip install piglet         # same as pyvirtualdisplay\n",
        "!pip install colabgymrender     # same as open gl and with this it will download that rendered file and then u can see what's going on in your enviroment locally rather than showing it on another window\n",
        "!pip install gym[atari]     # atari game enviroment, it is easier to plug and play or make changes in enviroment and to test model in your own way"
      ],
      "metadata": {
        "id": "r6AdWrd2MVYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display      # import and use virtual display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "wnrFMvuHO6Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "metadata": {
        "id": "R5b_JT4RO64X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"MountainCar-v0\")        # creating the gym enviroment whose name is Mountaincar-v0 and u can change these enviroment by changing the name here and checking in the gym.makeenviroment\n",
        "\n",
        "env.reset()\n",
        "#plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "metadata": {
        "id": "baS4iHgWPRfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs0 = env.reset()          # these are observations from that particular enviroment which can be used to make changes in the enviroment\n",
        "print(\"initial observation code:\", obs0)\n"
      ],
      "metadata": {
        "id": "tDNT28PoQPCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"taking action 2 (right)\")        #\n",
        "new_obs, reward, is_done, _ = env.step(2)\n",
        "\n",
        "print(\"new observation code:\", new_obs)\n",
        "print(\"reward:\", reward)\n",
        "print(\"is game over?:\", is_done)"
      ],
      "metadata": {
        "id": "grnQ85AtQmGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create env manually to set time limit. Please don't change this.\n",
        "from colabgymrender.recorder import Recorder      # it will help in record everything in the form of video\n",
        "TIME_LIMIT = 250      # intialising the time limit of 250 seconds\n",
        "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(),       # putting the controls for the enviroment like Q values\n",
        "                             max_episode_steps=TIME_LIMIT + 1)\n",
        "directory = './video'\n",
        "env = Recorder(env, directory)\n",
        "s = env.reset()\n",
        "actions = {'left': 0, 'stop': 1, 'right': 2}      # intial Q values \n",
        "\n",
        "# prepare \"display\"\n",
        "%matplotlib notebook\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "fig.show()\n",
        "\n",
        "def policy(t):      # function of policy\n",
        "    if t>50 and t<100:\n",
        "        return actions['left']\n",
        "    else:\n",
        "        return actions['right']\n",
        "\n",
        "\n",
        "for t in range(TIME_LIMIT):\n",
        "    \n",
        "    # change the line below to reach the flag\n",
        "    s, r, done, _ = env.step(policy(t))\n",
        "    \n",
        "    #draw game image on display\n",
        "    ax.clear()\n",
        "    ax.imshow(env.render('rgb_array'))\n",
        "    fig.canvas.draw()\n",
        "    \n",
        "    if done:\n",
        "        print(\"Well done!\")\n",
        "        break\n",
        "else:    \n",
        "    print(\"Time limit exceeded. Try again.\")"
      ],
      "metadata": {
        "id": "ctaXD_LOQpgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # now just go into the file section of colab and open the video folder and play that rendered video"
      ],
      "metadata": {
        "id": "79M1f-laSqgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}